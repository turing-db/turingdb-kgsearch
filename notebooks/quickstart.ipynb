{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d731d9cc-2ac6-4b5b-a7e0-6e95beaa7316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DiGraph with 3 nodes and 2 edges\n",
      "âœ“ Vector index built using context-enriched embeddings approach (strategy heavy): 3 vectors\n",
      "Building sparse index (TF-IDF)...\n",
      "  Using provided node_texts (from dense embeddings)\n",
      "âœ“ Sparse index built: 3 vectors\n",
      "  Vocabulary size: 3\n",
      "  Sample terms: ['ai', 'topic', 'topic ai']\n",
      "Training Node2Vec on graph structure...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dev/turingdb-kgsearch/.venv/lib/python3.13/site-packages/node2vec/edges.py:6: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n",
      "/home/dev/turingdb-kgsearch/.venv/lib/python3.13/site-packages/node2vec/edges.py:6: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n",
      "/home/dev/turingdb-kgsearch/.venv/lib/python3.13/site-packages/node2vec/edges.py:6: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Node2Vec trained: 3 structural vectors\n",
      "doc2: 1.000\n",
      "topic1: 0.593\n",
      "doc1: 0.300\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from turingdb_kgsearch.embeddings import build_context_enriched_embeddings, build_sparse_embeddings, build_node2vec_embeddings\n",
    "from turingdb_kgsearch.search import hybrid_search\n",
    "\n",
    "# Create knowledge graph\n",
    "G = nx.DiGraph()\n",
    "G.add_node(\"doc1\", type=\"document\", text=\"Machine learning basics\")\n",
    "G.add_node(\"doc2\", type=\"document\", text=\"Deep learning neural networks\")\n",
    "G.add_node(\"topic1\", type=\"topic\", name=\"AI\")\n",
    "G.add_edge(\"topic1\", \"doc1\", rel=\"contains\")\n",
    "G.add_edge(\"topic1\", \"doc2\", rel=\"contains\")\n",
    "print(G)\n",
    "\n",
    "# Build embeddings\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L3-v2')\n",
    "node_vectors_context_heavy, node_texts = build_context_enriched_embeddings(G, model, strategy=\"heavy\")\n",
    "node_vectors_sparse, _, vectorizer_sparse = build_sparse_embeddings(G, node_texts=node_texts)\n",
    "node_vectors_node2vec = build_node2vec_embeddings(G, dimensions=384)\n",
    "\n",
    "# Search\n",
    "results = hybrid_search(\n",
    "    query=\"introduction to neural networks\",\n",
    "    G=G,\n",
    "    dense_node_vectors=node_vectors_context_heavy,\n",
    "    sparse_node_vectors=node_vectors_sparse,\n",
    "    sparse_vectorizer=vectorizer_sparse,\n",
    "    node_texts=node_texts,  # same node texts used for both dense and sparse\n",
    "    model=model,\n",
    "    k=3,\n",
    "    alpha=0.7,  # 70% semantic, 30% keywords\n",
    ")\n",
    "\n",
    "for r in results[:3]:\n",
    "    print(f\"{r['node_id']}: {r['similarity']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8011f5f-e3b9-40c0-9c15-52372dbb8fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1: Hybrid search for 'risk management AI systems'...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Found 3 semantically relevant seed nodes:\n",
      "  1. topic1 (score: 1.000)\n",
      "     Topic: AI...\n",
      "  2. doc2 (score: 0.309)\n",
      "     doc2. under topic AI...\n",
      "  3. doc1 (score: 0.300)\n",
      "     doc1. under topic AI...\n",
      "\n",
      "================================================================================\n",
      "Stage 2: Hybrid filtering (structural + semantic)...\n",
      "  - Max hops: 2\n",
      "  - Min structural similarity: 0.7\n",
      "  - Min semantic similarity: 0.6\n",
      "  - Structural weight: 0.5\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "  Expanding from: topic1\n",
      "  Found 0 neighbors (after hybrid filtering):\n",
      "\n",
      "  Expanding from: doc2\n",
      "  Found 0 neighbors (after hybrid filtering):\n",
      "\n",
      "  Expanding from: doc1\n",
      "  Found 0 neighbors (after hybrid filtering):\n",
      "\n",
      "================================================================================\n",
      "Building subgraph...\n",
      "âœ“ Subgraph created:\n",
      "  Total nodes: 3\n",
      "  - Seed nodes: 3\n",
      "  - Found neighbors: 0\n",
      "  - Intermediate nodes: 0\n",
      "  Total edges: 2\n",
      "Found 3 nodes, 2 edges\n"
     ]
    }
   ],
   "source": [
    "from turingdb_kgsearch.workflow import search_and_expand_hybrid_filtered\n",
    "\n",
    "# Stage 1: Find seed nodes via hybrid search\n",
    "# Stage 2: Traverse graph with semantic+structural filtering\n",
    "semantic_results, expanded, subgraph = search_and_expand_hybrid_filtered(\n",
    "    query=\"risk management AI systems\",\n",
    "    G=G,\n",
    "    node_vectors=node_vectors_context_heavy,\n",
    "    node_texts=node_texts,\n",
    "    sparse_vectors=node_vectors_sparse,\n",
    "    sparse_vectorizer=vectorizer_sparse,\n",
    "    structural_vectors=node_vectors_node2vec,\n",
    "    model=model,\n",
    "    k_search=3,           # Find 3 seed nodes\n",
    "    max_hops=2,           # Explore 2 hops in graph\n",
    "    min_structural_sim=0.7,  # Structural similarity threshold\n",
    "    min_semantic_sim=0.6,    # Semantic similarity threshold\n",
    "    structural_weight=0.5,  # 50-50 balance (structure vs. semantic)\n",
    "    alpha=0.7,  # Weight alpha to attribute to semantic (dense) search, (1 - alpha) for keyword (sparse) search\n",
    ")\n",
    "\n",
    "# Subgraph contains relevant nodes + edges + similarity scores\n",
    "print(f\"Found {subgraph.number_of_nodes()} nodes, {subgraph.number_of_edges()} edges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c53d571-1843-4f96-8046-408c2820da3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SUBGRAPH STATISTICS\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š Basic Metrics:\n",
      "   nodes: 3\n",
      "   edges: 2\n",
      "   density: 0.3333333333333333\n",
      "   is_connected: True\n",
      "\n",
      "ðŸ”— Degree Statistics:\n",
      "   average: 1.33\n",
      "   max: 2.00\n",
      "   min: 1.00\n",
      "   median: 1.00\n",
      "\n",
      "ðŸ·ï¸  Node Types:\n",
      "   document: 2\n",
      "   topic: 1\n",
      "\n",
      "ðŸŽ¯ Node Roles:\n",
      "   seed: 3\n",
      "   found: 0\n",
      "   intermediate: 0\n",
      "\n",
      "â­ Most Central Nodes:\n",
      "   By Degree:\n",
      "      topic1: 1.000\n",
      "      doc1: 0.500\n",
      "      doc2: 0.500\n",
      "   By Betweenness:\n",
      "      doc1: 0.000\n",
      "      doc2: 0.000\n",
      "      topic1: 0.000\n",
      "\n",
      "ðŸ”— Edge Types:\n",
      "   contains: 2\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "from turingdb_kgsearch.statistics import get_subgraph_stats, print_subgraph_stats\n",
    "\n",
    "# Comprehensive statistics\n",
    "stats = get_subgraph_stats(subgraph, include_node_breakdown=True, include_centrality=True, include_paths=True)\n",
    "print_subgraph_stats(stats, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "607aa524-b000-457d-8f92-af5928209176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "NODE IMPORTANCE RANKINGS\n",
      "================================================================================\n",
      "\n",
      "Ranked 3 nodes using: pagerank, degree, betweenness, closeness, eigenvector\n",
      "\n",
      "\n",
      "ðŸ“Š PAGERANK (Top 5):\n",
      "   1. doc1 (document): 0.3701\n",
      "   2. doc2 (document): 0.3701\n",
      "   3. topic1 (topic): 0.2597\n",
      "\n",
      "ðŸ“Š DEGREE (Top 5):\n",
      "   1. topic1 (topic): 1.0000\n",
      "   2. doc1 (document): 0.5000\n",
      "   3. doc2 (document): 0.5000\n",
      "\n",
      "ðŸ“Š BETWEENNESS (Top 5):\n",
      "   1. doc1 (document): 0.0000\n",
      "   2. doc2 (document): 0.0000\n",
      "   3. topic1 (topic): 0.0000\n",
      "\n",
      "ðŸ“Š CLOSENESS (Top 5):\n",
      "   1. doc1 (document): 0.5000\n",
      "   2. doc2 (document): 0.5000\n",
      "   3. topic1 (topic): 0.0000\n",
      "\n",
      "ðŸ“Š EIGENVECTOR (Top 5):\n",
      "   1. doc1 (document): 0.7071\n",
      "   2. doc2 (document): 0.7071\n",
      "   3. topic1 (topic): 0.0015\n",
      "\n",
      "â­ COMBINED RANKING (Top 10):\n",
      "   1. doc1 (document): 0.6000\n",
      "   2. doc2 (document): 0.6000\n",
      "   3. topic1 (topic): 0.2000\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "from turingdb_kgsearch.ranking import rank_nodes_by_importance, print_node_rankings\n",
    "\n",
    "# Usage examples\n",
    "rankings = rank_nodes_by_importance(\n",
    "    subgraph,\n",
    "    methods=\"all\",  # or ['pagerank', 'degree', 'relevance']\n",
    "    top_k=10,\n",
    "    aggregate=\"average\",  # or 'max' or {'pagerank': 0.4, 'degree': 0.3, 'relevance': 0.3}\n",
    ")\n",
    "\n",
    "print_node_rankings(rankings, subgraph, show_details=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49b02197-86a9-4161-b31b-d191a33fccf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auto-generated color map for 2 node types:\n",
      "  document: #ff6b6b\n",
      "  topic: #4ecdc4\n",
      "âœ“ Interactive graph saved to: graph.html\n",
      "  Nodes: 3\n",
      "  Edges: 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"750\"\n",
       "            src=\"graph.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7d2af8348ec0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from turingdb_kgsearch.visualization import visualize_graph_with_pyvis\n",
    "\n",
    "# Interactive HTML visualization\n",
    "visualize_graph_with_pyvis(\n",
    "    subgraph,\n",
    "    output_file='graph.html'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "385ea19d-30b8-41af-9386-268929cfa4bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 'What are the key privacy requirements?'\n",
      "Stage 1: Hybrid search for 'What are the key privacy requirements?'...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Found 3 semantically relevant seed nodes:\n",
      "  1. doc2 (score: 1.000)\n",
      "     doc2. under topic AI...\n",
      "  2. doc1 (score: 0.653)\n",
      "     doc1. under topic AI...\n",
      "  3. topic1 (score: 0.300)\n",
      "     Topic: AI...\n",
      "\n",
      "================================================================================\n",
      "Stage 2: Hybrid filtering (structural + semantic)...\n",
      "  - Max hops: 10\n",
      "  - Min structural similarity: 0.1\n",
      "  - Min semantic similarity: 0.1\n",
      "  - Structural weight: 0.5\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "  Expanding from: doc2\n",
      "  Found 0 neighbors (after hybrid filtering):\n",
      "\n",
      "  Expanding from: doc1\n",
      "  Found 0 neighbors (after hybrid filtering):\n",
      "\n",
      "  Expanding from: topic1\n",
      "  Found 0 neighbors (after hybrid filtering):\n",
      "\n",
      "================================================================================\n",
      "Building subgraph...\n",
      "âœ“ Subgraph created:\n",
      "  Total nodes: 3\n",
      "  - Seed nodes: 3\n",
      "  - Found neighbors: -2\n",
      "  - Intermediate nodes: 0\n",
      "  Total edges: 2\n",
      "================================================================================\n",
      "HYBRID-FILTERED WORKFLOW RESULTS\n",
      "================================================================================\n",
      "\n",
      "1. SEED NODE (Hybrid Search Match):\n",
      "   Node: doc2\n",
      "   Semantic Score: 1.000\n",
      "   Text: doc2. under topic AI...\n",
      "\n",
      "   HYBRID-FILTERED NEIGHBORS:\n",
      "   (Must pass BOTH structural AND semantic thresholds)\n",
      "\n",
      "2. SEED NODE (Hybrid Search Match):\n",
      "   Node: doc1\n",
      "   Semantic Score: 0.653\n",
      "   Text: doc1. under topic AI...\n",
      "\n",
      "   HYBRID-FILTERED NEIGHBORS:\n",
      "   (Must pass BOTH structural AND semantic thresholds)\n",
      "\n",
      "3. SEED NODE (Hybrid Search Match):\n",
      "   Node: topic1\n",
      "   Semantic Score: 0.300\n",
      "   Text: Topic: AI...\n",
      "\n",
      "   HYBRID-FILTERED NEIGHBORS:\n",
      "   (Must pass BOTH structural AND semantic thresholds)\n"
     ]
    }
   ],
   "source": [
    "from turingdb_kgsearch.workflow import search_and_expand_hybrid_filtered, generate_report_hybrid_workflow_results\n",
    "\n",
    "query = \"What are the key privacy requirements?\"\n",
    "print(f\"Query: '{query}'\")\n",
    "\n",
    "semantic_results, expanded, subgraph = search_and_expand_hybrid_filtered(\n",
    "    query=query,\n",
    "    G=G,\n",
    "    node_vectors=node_vectors_context_heavy,\n",
    "    node_texts=node_texts,\n",
    "    sparse_vectors=node_vectors_sparse,\n",
    "    sparse_vectorizer=vectorizer_sparse,\n",
    "    structural_vectors=node_vectors_node2vec,\n",
    "    model=model,\n",
    "    k_search=5,\n",
    "    max_hops=10,\n",
    "    min_structural_sim=0.1,  # Must be structurally similar\n",
    "    min_semantic_sim=0.1,  # AND semantically relevant to query\n",
    "    structural_weight=0.5,  # 50-50 balance\n",
    "    alpha=0.7,  # Weight alpha to attribute to semantic (dense) search, (1 - alpha) for keyword (sparse) search\n",
    ")\n",
    "\n",
    "report = generate_report_hybrid_workflow_results(semantic_results, expanded)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c29f62c-2afa-4801-8bc2-23eed1340d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "api_keys = {\n",
    "    \"Anthropic\": os.getenv(\"ANTHROPIC_API_KEY\"),\n",
    "    \"Mistral\": os.getenv(\"MISTRAL_API_KEY\"),\n",
    "    \"OpenAI\": os.getenv(\"OPENAI_API_KEY\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d907be8-3426-4f43-9110-3c291cbb83e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 'What are the key privacy requirements?'\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Based on the provided graph data, I cannot definitively answer the specific query about \"key privacy requirements\" because:\n",
       "\n",
       "1. The graph structure lacks explicit privacy-related content\n",
       "2. The available nodes (doc1, doc2, topic1) do not show clear privacy details\n",
       "3. The semantic scores suggest some relevance to AI, but no specific privacy information is visible\n",
       "\n",
       "Analysis of Available Information:\n",
       "- The graph contains 3 nodes (2 documents, 1 topic)\n",
       "- The topic is related to AI\n",
       "- Documents are connected to the AI topic\n",
       "- Semantic scores range from 0.300 to 1.000\n",
       "\n",
       "Recommendation:\n",
       "To properly answer the privacy requirements query, additional graph context or document content would be needed. The current graph does not provide sufficient information to extract privacy-specific insights.\n",
       "\n",
       "Limitations of Current Response:\n",
       "- Insufficient detail about privacy requirements\n",
       "- No direct privacy-related nodes or edges\n",
       "- Need more granular document or topic information\n",
       "\n",
       "Suggested Next Steps:\n",
       "- Expand graph with more detailed nodes\n",
       "- Include specific privacy-related content\n",
       "- Provide fuller document text for semantic analysis"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from turingdb_kgsearch.llm import graph_to_llm_context, create_llm_prompt_with_graph, query_llm\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Convert graph to LLM-friendly format\n",
    "graph_text = graph_to_llm_context(subgraph, format='natural')\n",
    "\n",
    "# Create complete prompt\n",
    "print(f\"Query: '{query}'\")\n",
    "prompt = create_llm_prompt_with_graph(\n",
    "    query=query,\n",
    "    subgraph=subgraph,\n",
    "    report=report,\n",
    "    format='natural'\n",
    ")\n",
    "\n",
    "# Send to your LLM\n",
    "provider = \"Anthropic\"\n",
    "\n",
    "result = query_llm(\n",
    "    prompt=prompt,\n",
    "    provider=provider,\n",
    "    api_key=api_keys[provider],\n",
    "    temperature=0.2,\n",
    ")\n",
    "display(Markdown(result))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
